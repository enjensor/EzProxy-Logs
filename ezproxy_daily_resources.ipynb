{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EzProxy Daily : Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This runs best in Jupyter, either on a local machine or on a server you have file access to.*** \n",
    "\n",
    "To review total use of EzProxy, make sure you place the audit logs into the /data folder and that they are named in the syntax of \"YYYYMMDD.txt\" (for example, \"20190314.txt\"). These audit files are usually in the /audit sub-folder of your EzProxy application folder on the server. Your audit logs will need to be in the following format:\n",
    "\n",
    "> **%h %{ezproxy-session}i %u %t \"%r\" %s %b**\n",
    "\n",
    "Please also maked sure that you place the EzProxy log files in the /data_e folder and that they are named in the syntax of \"ezproxyYYYYMMDD.log\" (for example, \"ezproxy20190101.log\"). These ezproxy files are usually in the /log sub-folder of your EzProxy application folder on the server. Your EzProxy logs will need to be in the following format, which is slightly different from the audit log format:\n",
    "\n",
    "> **%h %{ezproxy-session}i %U %u %t \"%r\" %s %b**\n",
    "\n",
    "Once you have some files in the approprate data folders, *run cells 1 through to 9*. If there are no warnings or errors, then you will be presented with a calendar dropdown menu, from which you can select the date for review. Once you select a date, it will read the logs and provide options for the following views. Keep in mind that because there will be thousands of IP addresses to query, it will take a very long time to obtain results. Best to run the script and come back to it an hour later.\n",
    "\n",
    "![Daily Resources: Birds' Eye View](./docs/daily_resources_reduced.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activate all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ezproxy_daily_resources.ipynb to script\n",
      "[NbConvertApp] Writing 24217 bytes to ezproxy_daily_resources.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!jupyter nbconvert --to script ezproxy_daily_resources.ipynb\n",
    "os.rename(\"./ezproxy_daily_resources.py\", \"./py/ezproxy_daily_resources.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRESH ANACONDA INSTALL\n",
    "# export PATH=/anaconda3/bin:$PATH\n",
    "# conda config --add channels conda-forge\n",
    "# conda install -c conda-forge proj4\n",
    "# conda install -c anaconda mysql-connector-python\n",
    "# conda install -c conda-forge cartopy\n",
    "# conda install -c conda-forge tldextract\n",
    "# conda install -c conda-forge basemap\n",
    "# conda install -c conda-forge basemap-data-hires\n",
    "# conda install -c conda-forge ipywidgets\n",
    "# conda install -c conda-forge folium\n",
    "# conda install -c conda-forge pyzmq\n",
    "# conda install -c conda-forge jupyterlab\n",
    "# conda install -c conda-forge nodejs\n",
    "# conda install python=3.6.7\n",
    "# jupyter nbextension enable --py widgetsnbextension\n",
    "# jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "# jupyter lab build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import mysql.connector\n",
    "import matplotlib.dates as mdates\n",
    "import cartopy.crs as ccrs\n",
    "os.environ['PROJ_LIB'] = '/anaconda3/share/proj'\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import tldextract\n",
    "from datetime import datetime, timedelta, date\n",
    "from ipywidgets import interact, interactive, interact_manual, Button, HBox, VBox, Layout, ButtonStyle\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_date(change):\n",
    "    global sessDate\n",
    "    global sessdown\n",
    "    global appsdown\n",
    "    global errorsA\n",
    "    global errorsB\n",
    "    global errorsC\n",
    "    global df\n",
    "    global df5\n",
    "    global df6\n",
    "    global pf\n",
    "    global data2\n",
    "    global thisDate3\n",
    "    global today\n",
    "    errorsA = []\n",
    "    errorsB = []\n",
    "    errorsC = []\n",
    "    today = date.today()\n",
    "    today = int(today.strftime(\"%Y%m%d\"))\n",
    "    thisDate = aDates.value\n",
    "    thisDate2 = str(aDates.value)\n",
    "    thisDate3 = int(thisDate.strftime(\"%Y%m%d\"))\n",
    "    sessDate = thisDate.strftime(\"%Y%m%d\")\n",
    "    if thisDate3  > today:\n",
    "        thisDate = date.today()\n",
    "    with outB:\n",
    "        clear_output()\n",
    "    with outC:\n",
    "        clear_output()\n",
    "    with outCa:\n",
    "        clear_output()\n",
    "    with outD:\n",
    "        clear_output()\n",
    "        print(\"\\t\" + str(sessDate) + \" : This may take a few minutes ...\",end=\"\\n\\n\")\n",
    "    with outE:\n",
    "        clear_output()\n",
    "    with outG:\n",
    "        clear_output()\n",
    "    with outH:\n",
    "        clear_output()\n",
    "    with outJ:\n",
    "        clear_output()\n",
    "    with outK:\n",
    "        clear_output()\n",
    "    try:\n",
    "        with outD:\n",
    "            print(\"\\tLoading log ...\",end=\"\\n\\n\")\n",
    "        data = pd.read_csv(\"./data_e/ezproxy\" + sessDate + \".log\", sep=\" \", header=None, error_bad_lines=False, warn_bad_lines=False)\n",
    "        data.columns = [\"ipaddress\", \"sessionid\", \"url\", \"urlsessionid\", \"adate\", \"azone\", \"adomain\", \"astatus\", \"asize\"]\n",
    "        data2 = pd.DataFrame(data)\n",
    "        data2.reset_index(drop=True, inplace=True)\n",
    "        with outD:\n",
    "            print(\"\\tParsing sessions ...\",end=\"\\n\\n\")\n",
    "        data3 = {\"ipaddress\":[], \"sessionid\":[], \"url\":[], \"time\":[], \"size\":[]}\n",
    "    except ValueError as e:\n",
    "        errorsA.append(\"Catch 1 E: \" + str(e))\n",
    "    except IOError as e:\n",
    "        errorsA.append(\"Catch 1a E: \" + str(e))\n",
    "    except:\n",
    "        errorsA.append(\"Catch 1b E: \", sys.exc_info()[0])     \n",
    "    with outD:\n",
    "        print(\"\\tIterate over rows ...\",end=\"\\n\\n\")\n",
    "    try:\n",
    "        for i in data2.index:\n",
    "            adate = data2.get_value(i,'adate')\n",
    "            asize = data2.get_value(i,'asize')\n",
    "            sessionid = data2.get_value(i,'sessionid')\n",
    "            ipaddress = data2.get_value(i,'ipaddress')\n",
    "            domain = data2.get_value(i,'url')\n",
    "            matches = re.search(\"login\", domain)\n",
    "            matchesb = re.search(\"http[s]?://ezproxy.uws.edu.au\", domain)\n",
    "            adatex, datex = re.split(\"\\[\", adate)\n",
    "            datex = pd.to_datetime(datex, format=\"%d/%b/%Y:%H:%M:%S\")\n",
    "            datex = datex.strftime('%H:%M')\n",
    "            if domain is not None and domain != \"\" and domain != \"-\" and matches is None and matchesb is None:\n",
    "                try:\n",
    "                    dom, domy = re.split(\".ezproxy.uws.edu.au\", domain)\n",
    "                except ValueError as e:\n",
    "                    domain = re.sub(\"\\-\",\".\",str(domain))\n",
    "                    extracted = tldextract.extract(domain)\n",
    "                    domain = extracted.domain\n",
    "                    errorsB.append(str(domain))\n",
    "                    dom = None\n",
    "                except IOError as e:\n",
    "                    errorsB.append(\"Catch 2 E: \" + str(e))\n",
    "                    dom = None\n",
    "                except:\n",
    "                    errorsB.append(\"Catch 2a E: \", sys.exc_info()[0])\n",
    "                    dom = None\n",
    "            else:\n",
    "                if domain is not None and domain != \"\" and domain != \"-\":\n",
    "                    domain = re.sub(\"\\-\",\".\",str(domain))\n",
    "                    extracted = tldextract.extract(domain)\n",
    "                    domain = extracted.domain\n",
    "                    errorsB.append(str(domain))\n",
    "                dom = None\n",
    "            try:\n",
    "                if str(sessionid) != \"-\" and str(sessionid) != \"\" and str(dom) != \"\" and str(dom) != \"None\":\n",
    "                    dom = re.sub(\"\\-\",\".\",str(dom))\n",
    "                    extracted = tldextract.extract(dom)\n",
    "                    dom = extracted.domain\n",
    "                    data3[\"ipaddress\"].append(str(ipaddress))\n",
    "                    data3[\"sessionid\"].append(str(sessionid))\n",
    "                    data3[\"url\"].append(str(dom))\n",
    "                    data3[\"time\"].append(datex)\n",
    "                    data3[\"size\"].append(asize)\n",
    "                else:\n",
    "                    if matches is not None or matchesb is not None:\n",
    "                        errorsB.append(\"ezproxy\")\n",
    "                    else:\n",
    "                        domain = re.sub(\"\\-\",\".\",str(data2.get_value(i,'url')))\n",
    "                        extracted = tldextract.extract(domain)\n",
    "                        domain = extracted.domain\n",
    "                        errorsB.append(str(domain))\n",
    "            except ValueError as v:\n",
    "                errorsB.append(\"Catch 2b E: \" + str(v))\n",
    "            except IOError as v:\n",
    "                errorsB.append(\"Catch 2c E: \" + str(v))\n",
    "            except:\n",
    "                errorsB.append(\"Catch 2d E: sess \" + str(type(sessionid)) + \" dom \" + str(type(dom)) + \" error \" + str(sys.exc_info()[0]))\n",
    "    except ValueError as z:\n",
    "        errorsC.append(\"Catch 3 E: \" + str(z))\n",
    "    except IOError as z:\n",
    "        errorsC.append(\"Catch 3b E: \" + str(z))\n",
    "    except:\n",
    "        errorsC.append(\"Catch 3c E: \", sys.exc_info()[0])\n",
    "    with outD:\n",
    "        print(\"\\n\\n\\tGrouping and counting results\",end=\"\\n\\n\")\n",
    "    try:\n",
    "        df = pd.DataFrame(data3)\n",
    "        df3 = df.groupby(['url'], as_index=False)['size'].agg(['sum','count'])\n",
    "        df4 = df.groupby(['url'], as_index=False)['size'].agg(['count'])\n",
    "        df5 = df.groupby(['ipaddress'], as_index=False)['size'].agg(['count','sum'])\n",
    "        df6 = df4[df4[\"count\"] > 250]\n",
    "    except ValueError as e:\n",
    "        errorsC.append(\"Catch 3d E: \" + str(e))\n",
    "    except IOError as e:\n",
    "        errorsC.append(\"Catch 3e E: \" + str(e))\n",
    "    except:\n",
    "        errorsC.append(\"Catch 3f E: \", sys.exc_info()[0])\n",
    "    dfObj = pd.DataFrame(errorsB, columns = ['url'])\n",
    "    dfObj2 = dfObj.groupby(['url'], as_index=False)['url'].agg(['count'])\n",
    "    dfObj2.drop(dfObj2[dfObj2['count'] < 2].index, inplace=True)\n",
    "    dfObj2 = dfObj2.sort_values('count',ascending=False)\n",
    "#    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#        with outF:\n",
    "#            clear_output()\n",
    "#            display(HTML('<h4>Other Sites</h4>'))\n",
    "#            display(dfObj2)\n",
    "    with outD:\n",
    "        clear_output()\n",
    "        fig, ax = plt.subplots(figsize=(15,7))\n",
    "        ax.set_title('EzProxy Resource Access | '+sessDate,y=1.08)\n",
    "        fig.tight_layout()\n",
    "        pff = df4.index.tolist()\n",
    "        pf = df6.index.tolist()\n",
    "        pv = []\n",
    "        for sublist in df4.values:\n",
    "            for item in sublist:\n",
    "                pv.append(item)\n",
    "        plt.ylim([0,45000])\n",
    "        plt.xticks(fontsize=9, rotation=90)\n",
    "        plt.bar(pff,pv)\n",
    "        plt.savefig('./imgs/ezproxy_resources_'+sessDate+'.png', bbox_inches = \"tight\")\n",
    "        plt.show()\n",
    "    with outB:\n",
    "        clear_output()\n",
    "        sessdown = widgets.Button(description=\"View Locations\",layout=Layout(width=\"240px\"))\n",
    "        display(sessdown)\n",
    "        sessdown.on_click(on_world)\n",
    "    with outC:\n",
    "        clear_output()\n",
    "        appsdown = widgets.Button(description=\"View Hourly Use\",layout=Layout(width=\"240px\"))\n",
    "        display(appsdown)\n",
    "        appsdown.on_click(on_hours)\n",
    "    with outCa:\n",
    "        clear_output()\n",
    "        appsdown = widgets.Button(description=\"View Hourly Load\",layout=Layout(width=\"240px\"))\n",
    "        display(appsdown)\n",
    "        appsdown.on_click(on_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_hours(b):\n",
    "    global publishers\n",
    "    global raw_data_list\n",
    "    global raw_data\n",
    "    global colors\n",
    "    global i_values\n",
    "    global a_values\n",
    "    global downloads\n",
    "    hours = ['00','01','02','03','04','05','06','07','08','09','10','11','12',\n",
    "         '13','14','15','16','17','18','19','20','21','22','23']\n",
    "    raw_data = pd.DataFrame(columns=['platform','platformValue'])\n",
    "    raw_data_list = []\n",
    "    publishers = pf\n",
    "    for p in publishers:\n",
    "        p_values = []\n",
    "        for h in hours:\n",
    "            dpp = df\n",
    "            aa = dpp[(dpp[\"url\"] == p) & (dpp[\"time\"].str.contains(h+\":\\d\\d\",regex=True))].count()\n",
    "            aa = aa[\"url\"]\n",
    "            p_values.append(aa)\n",
    "        raw_data2 = raw_data.append({'platform':p,'platformValue':p_values}, ignore_index=True)\n",
    "        raw_data = raw_data2\n",
    "        raw_data_list.append(p_values)\n",
    "    c = dict()\n",
    "    r = lambda: random.randint(0,255)\n",
    "    for i in range(0, len(publishers)+1):\n",
    "        d = ('#%02X%02X%02X' % (r(),r(),r()))\n",
    "        c[i] = d\n",
    "    colors = c\n",
    "    with outG:\n",
    "        clear_output()\n",
    "        matplotlib.rc('font', serif='Helvetica Neue')\n",
    "        matplotlib.rcParams.update({'font.size': 10})\n",
    "        fig = plt.gcf()\n",
    "        fig, ax = plt.subplots(figsize=(18,20));\n",
    "        N = len(hours)\n",
    "        ind = np.arange(N)\n",
    "        width = 0.75\n",
    "        pi = []\n",
    "        for i in range(0,len(publishers)):\n",
    "            h = i-1\n",
    "            t = publishers[i]\n",
    "            if(i == 0):\n",
    "                h = 0\n",
    "                plt.bar(ind, raw_data_list[i], width, color=c[i])\n",
    "            else:\n",
    "                plt.bar(ind, raw_data_list[i], width, bottom=raw_data_list[h], color=c[i])\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.ylim([0,11000])\n",
    "        plt.xticks(ind, hours, fontsize=12, rotation=45)\n",
    "        ax.set_title('EzProxy Resource Use | '+sessDate,y=1.08)\n",
    "        leg = plt.legend(publishers, fontsize=12, ncol=2, framealpha=0, fancybox=True, loc='upper left')\n",
    "        legc = leg.legendHandles\n",
    "        for i in range(0,len(publishers)):\n",
    "            legc[i].set_color(c[i])\n",
    "        plt.savefig('./imgs/ezproxy_resources_use'+sessDate+'.png', bbox_inches = \"tight\")\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_world(b):\n",
    "    global logs\n",
    "    global logs2\n",
    "    global data\n",
    "    ips = df5.index.tolist()\n",
    "    thisFile = \"./outputs/resources_\" + sessDate + \"_log.csv\"\n",
    "    with open(thisFile, mode='w') as audit_file:\n",
    "        audit_writer = csv.writer(audit_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        audit_writer.writerow([\"IP\",\"lat\",\"lon\",\"city\",\"dsize\",\"continent_name\",\n",
    "                               \"threat_is_tor\",\"threat_is_proxy\",\n",
    "                               \"threat_is_anonymous\",\"threat_is_known_attacker\",\n",
    "                               \"threat_is_known_abuser\",\"threat_is_threat\",\"threat_is_bogon\"])\n",
    "        z = 0\n",
    "        for x in ips:\n",
    "            url = \"http://ip-api.com/json/\" + x\n",
    "            #apikey = \"\"\n",
    "            #url = \"https://api.ipdata.co/\" + x + \"?api-key=\" + apikey\n",
    "            r = requests.get(url)\n",
    "            results = r.json()\n",
    "            if len(results) > 1:\n",
    "                lat = results['lat']\n",
    "                lon = results['lon']\n",
    "                city = results['city']\n",
    "                #lat = results['latitude']\n",
    "                #lon = results['longitude']\n",
    "                #continent_name = results['continent_name']\n",
    "                #threat_is_tor = results['threat']['is_tor']\n",
    "                #threat_is_proxy = results['threat']['is_proxy']\n",
    "                #threat_is_anonymous = results['threat']['is_anonymous']\n",
    "                #threat_is_known_attacker = results['threat']['is_known_attacker']\n",
    "                #threat_is_known_abuser = results['threat']['is_known_abuser']\n",
    "                #threat_is_threat = results['threat']['is_threat']\n",
    "                #threat_is_bogon = results['threat']['is_bogon']\n",
    "                dsize = df5.iloc[z][\"sum\"]\n",
    "                continent_name = \"\"\n",
    "                threat_is_tor = \"\"\n",
    "                threat_is_proxy = \"\"\n",
    "                threat_is_anonymous = \"\"\n",
    "                threat_is_known_attacker = \"\"\n",
    "                threat_is_known_abuser = \"\"\n",
    "                threat_is_threat = \"\"\n",
    "                threat_is_bogon = \"\"\n",
    "                audit_writer.writerow([x,lat,lon,city,dsize,continent_name,\n",
    "                                       threat_is_tor,threat_is_proxy,threat_is_anonymous,\n",
    "                                       threat_is_known_attacker,threat_is_known_abuser,\n",
    "                                       threat_is_threat,threat_is_bogon])\n",
    "            z = z + 1\n",
    "            time.sleep(0.8)\n",
    "            #time.sleep(0.2) \n",
    "    logs = pd.read_csv(thisFile)\n",
    "    del logs['IP']\n",
    "    del logs['threat_is_tor']\n",
    "    del logs['threat_is_proxy']\n",
    "    del logs['threat_is_anonymous']\n",
    "    del logs['threat_is_known_attacker']\n",
    "    del logs['threat_is_known_abuser']\n",
    "    del logs['threat_is_threat']\n",
    "    del logs['threat_is_bogon']\n",
    "    logs2 = logs\n",
    "    logs3 = logs\n",
    "    logs = logs.groupby(['lat','lon','city','continent_name'], as_index=False)['city'].agg(['count'])\n",
    "    plt.figure(figsize=(18,18))\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.set_title('EzProxy International Access | ' + sessDate,y=1.08)\n",
    "    ax.set_global()\n",
    "    ax.coastlines(linewidth=0.6)\n",
    "    ax.stock_img()\n",
    "    ax.gridlines(xlocs=range(-180,181,40), ylocs=range(-80,81,20),draw_labels=False)\n",
    "    ax.gridlines(xlocs=range(-140,181,40), ylocs=range(-80,81,20),draw_labels=True)\n",
    "    ax.text(-0.05,0,'Latitude', transform=ax.transAxes, rotation='vertical', va='bottom')\n",
    "    ax.text(0,-0.07,'Longitude', transform=ax.transAxes, ha='left')\n",
    "    i = 0\n",
    "    imax = max(logs.values)\n",
    "    for row in logs.index:\n",
    "        lat = row[0]\n",
    "        lon = row[1]\n",
    "        latx = lat - 1.5\n",
    "        lonx = lon + 3.5\n",
    "        city = row[2]\n",
    "        magnify = logs.iloc[i].values\n",
    "        msize = int((100 / imax) * magnify)\n",
    "        if msize < 10:\n",
    "            msize = 10\n",
    "        if msize > 50:\n",
    "            msize = 50\n",
    "        i = i + 1\n",
    "        ax.plot(lon, lat, marker=\"o\", markersize=10, markerfacecolor=(1,0,0,1.0),\n",
    "                markeredgecolor=(1,0,0,1.0),markeredgewidth=0.1,fillstyle=\"full\")\n",
    "    with outE:\n",
    "        clear_output()\n",
    "        plt.savefig('./imgs/ezproxy_resource_access_'+sessDate+'.png', bbox_inches = \"tight\")\n",
    "        plt.show();\n",
    "    #data = logs2.groupby(['lat','lon','city','continent_name'], as_index=False)['dsize'].agg(['count'])\n",
    "    data = logs2.groupby(['lat','lon','city','continent_name'], as_index=False)['dsize'].agg(['sum'])\n",
    "    data['labels_enc'] = pd.factorize(data.index.get_level_values('continent_name'))[0]\n",
    "    plt.figure(figsize=(18,18))\n",
    "    #m = Basemap(llcrnrlon = -180, llcrnrlat = -80, urcrnrlon = 180, urcrnrlat = 80)\n",
    "    m = Basemap(lat_0=-31.840233, lon_0=146.9211)\n",
    "    #m.drawmapboundary(fill_color = '#f9fafa', linewidth = 0)\n",
    "    #m.fillcontinents(color = '#b1c8b0', alpha = 0.5)\n",
    "    m.shadedrelief()\n",
    "    #m.drawcoastlines(linewidth = 0.0, color = \"black\")\n",
    "    m.scatter(data.index.get_level_values('lon'), \n",
    "              data.index.get_level_values('lat'), \n",
    "              s = data.values/60000, alpha=0.5, \n",
    "              #s = data.values*20, alpha=0.5,\n",
    "              c = data['labels_enc'], \n",
    "              cmap = \"Set1\")\n",
    "    plt.title('EzProxy Resource Load Distribution | ' + sessDate,y=1.08)\n",
    "    with outH:\n",
    "        clear_output()\n",
    "        plt.savefig('./imgs/ezproxy_distribution_'+sessDate+'.png', bbox_inches = \"tight\")\n",
    "        plt.show();\n",
    "    plt.figure(figsize=(18, 15))\n",
    "    m = Basemap(projection=\"lcc\", width=7E6, height=5E6, lat_0=-29, lon_0=141)\n",
    "    m.shadedrelief()\n",
    "    data = logs3.groupby(['lat','lon','city','continent_name'], as_index=False)['dsize'].agg(['count'])\n",
    "    lat = data.index.get_level_values('lat')\n",
    "    lon = data.index.get_level_values('lon')\n",
    "    for i in range(0,len(lat)-1):\n",
    "        x,y = m(lon[i],lat[i])\n",
    "        m.plot(x, y, 'or', markersize=7, alpha=0.5)\n",
    "    plt.title('EzProxy Australian Access | ' + sessDate,y=1.05)\n",
    "    with outJ:\n",
    "        clear_output()\n",
    "        plt.savefig('./imgs/ezproxy_australia_'+sessDate+'.png', bbox_inches = \"tight\")\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_stacked(b):\n",
    "    global ti_values\n",
    "    global ta_values\n",
    "    global tdownloads\n",
    "    with outK:\n",
    "        hours = ['00','01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23']\n",
    "        blacklist = pd.read_csv(\"./data_ips/blacklist.txt\", sep=\",\", header=None, error_bad_lines=False, warn_bad_lines=False)\n",
    "        blacklist.columns = [\"IRN\",\"IPAddress\",\"OrganisationName\",\"ReportedBy\",\"ThreatType\",\"IsWhiteListed\",\"DateCreated\",\"Notes\"]\n",
    "        blackips = blacklist[\"IPAddress\"]\n",
    "        data3 = pd.DataFrame(data2)\n",
    "        data3.drop('azone', axis=1, inplace=True)\n",
    "        data3.drop('url', axis=1, inplace=True)\n",
    "        data3.drop('astatus', axis=1, inplace=True)\n",
    "        data3.drop('sessionid', axis=1, inplace=True)\n",
    "        data4 = data3[\"adate\"].str.split(pat = \":\", expand=True)\n",
    "        data3[\"adate\"] = data4.loc[:,1]\n",
    "        data3[\"asize\"] = round(data3[\"asize\"] / 1024 / 1024)\n",
    "        ips = data3.groupby(['ipaddress','adate'], as_index=True)['adate'].agg(['count']).index.tolist()\n",
    "        counts = data3.groupby(['ipaddress','adate'], as_index=True)['adate'].agg(['count']).values.tolist()\n",
    "        downloads = data3.groupby(['adate'], as_index=True)['asize'].sum().values.tolist()\n",
    "        d = (24 - len(downloads))\n",
    "        if d > 0:\n",
    "            for q in range(0,d):\n",
    "                qq = 0\n",
    "                downloads.append(qq)\n",
    "        df = pd.DataFrame(np.array(ips),columns=[\"ipaddress\",\"dates\"])\n",
    "        df[\"counts\"] = counts\n",
    "        i_values = []\n",
    "        a_values = []\n",
    "        for h in hours: \n",
    "            ii = df[(df[\"dates\"].str.contains(h,regex=True))].count()\n",
    "            if ii[\"ipaddress\"] != \"\" and ii[\"ipaddress\"] is not None:\n",
    "                ii = ii[\"ipaddress\"]\n",
    "            else:\n",
    "                ii = 0       \n",
    "            i_values.append(ii)\n",
    "            aa = df[(df[\"dates\"].str.contains(h,regex=True))].sum()\n",
    "            if aa[\"counts\"] != \"\" and aa[\"counts\"] is not None:\n",
    "                try:\n",
    "                    aa = round(sum(aa[\"counts\"])/100)\n",
    "                except ValueError as e:\n",
    "                    aa = 0\n",
    "                except IOError as e:\n",
    "                    aa = 0\n",
    "                except:\n",
    "                    aa = 0\n",
    "            else:\n",
    "                aa = round(0)\n",
    "            a_values.append(aa)\n",
    "        x = hours\n",
    "        y = [i_values,a_values,downloads]\n",
    "        ti_values = i_values\n",
    "        ta_values = a_values\n",
    "        tdownloads = downloads\n",
    "        plt.figure(figsize=(18,10))\n",
    "        plt.ylim([0,4000])\n",
    "        plt.stackplot(x,y, labels=['Unique IPs','Unique Web Requests (x100)','Total Traffic (Mb)'])\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.title('EzProxy Hourly Load | ' + sessDate,y=1.08)\n",
    "        clear_output()\n",
    "        plt.savefig('./imgs/ezproxy_daily_stacked_'+sessDate+'.png', bbox_inches = \"tight\")\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global aDates\n",
    "now = datetime.utcnow() - timedelta(days=1)\n",
    "aDates = widgets.DatePicker(\n",
    "    description='Log Date',\n",
    "    disabled=False,\n",
    "    value=datetime(now.year,now.month,now.day)\n",
    ")\n",
    "aDates.observe(on_date,names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b824ef90ad104859954d46392a8f7e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='0px solid #777777', height='2.3em', padding='0px', width='99%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85797c40a5ff4d58b95941f87bdae45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(border='0px solid #777777', height='2.3em', padding='0px', width='310px'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d6180625fe482ebb55c0a150ea789e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='0px solid #777777', height='650px', overflow='hidden', padding='0px', top='25px',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be07e965fa784d809d78a8d5e4397f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='0px solid #777777', height='650px', overflow='hidden', padding='0px', top='25px',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04684f3e8f045a1b0d6559f957d7951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='0px solid #777777', height='650px', left='43px', overflow='hidden', padding='0px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504d3cc4986b487ea009cd2d7dcfefc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='0px solid #777777', height='800px', left='43px', overflow='hidden', padding='0px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4ff4b999634b53bc40d1a8a122f061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='0px solid #777777', height='1330px', overflow='hidden', padding='0px', top='25px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b860c720bb9046fea4f8c7c90ebb9f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='0px solid #777777', height='650px', overflow='hidden', padding='0px', top='5px', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outZ = widgets.Output(layout={'border': '0px solid #777777', 'height':'2.3em', 'padding': '0px', 'width':'99%'})\n",
    "outA = widgets.Output(layout={'border': '0px solid #777777', 'height':'2.3em', 'padding': '0px', 'width':'310px'})\n",
    "outB = widgets.Output(layout={'border': '0px solid #777777', 'height':'2.3em', 'padding': '0px', 'width':'250px'})\n",
    "outC = widgets.Output(layout={'border': '0px solid #777777', 'height':'2.3em', 'padding': '0px', 'width':'250px'})\n",
    "outCa = widgets.Output(layout={'border': '0px solid #777777', 'height':'2.3em', 'padding': '0px', 'width':'250px'})\n",
    "outD = widgets.Output(layout={'border': '0px solid #777777', 'height':'650px', 'padding': '0px', 'width':'99%', 'top':'25px', 'overflow':'hidden'})\n",
    "outE = widgets.Output(layout={'border': '0px solid #777777', 'height':'650px', 'padding': '0px', 'width':'99%', 'top':'25px', 'overflow':'hidden'})\n",
    "outH = widgets.Output(layout={'border': '0px solid #777777', 'height':'650px', 'padding': '0px', 'width':'99%', 'top':'25px', 'overflow':'hidden', 'left':'43px'})\n",
    "outJ = widgets.Output(layout={'border': '0px solid #777777', 'height':'800px', 'padding': '0px', 'width':'99%', 'top':'25px', 'overflow':'hidden', 'left':'43px'})\n",
    "outG = widgets.Output(layout={'border': '0px solid #777777', 'height':'1330px', 'padding': '0px', 'width':'99%', 'top':'25px', 'overflow':'hidden'})\n",
    "outF = widgets.Output(layout={'border': '0px solid #777777', 'height':'800px', 'padding': '0px', 'width':'99%', 'top':'35px', 'overflow':'hidden'})\n",
    "outK = widgets.Output(layout={'border': '0px solid #777777', 'height':'650px', 'padding': '0px', 'width':'99%', 'top':'5px', 'overflow':'hidden'})\n",
    "interface = HBox([outA,outB,outC,outCa])\n",
    "interfaceb = HBox([outE,outF])\n",
    "display(outZ)\n",
    "display(interface)\n",
    "display(outD)\n",
    "display(outE)\n",
    "display(outH)\n",
    "display(outJ)\n",
    "display(outG)\n",
    "display(outK)\n",
    "with outA:\n",
    "    clear_output()\n",
    "    display(aDates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import imageio\n",
    "#import fnmatch\n",
    "#import moviepy.editor as mp\n",
    "#folders = [\"ezproxy_australia\",\"ezproxy_daily_stacked\",\"ezproxy_distribution\",\n",
    "#           \"ezproxy_resource_access\",\"ezproxy_resources_2019\",\"ezproxy_resources_use\"]\n",
    "#for f in folders:\n",
    "#    print(f)\n",
    "#    images = []\n",
    "#    for file in sorted(os.listdir('./imgs')):\n",
    "#        if fnmatch.fnmatch(file, f+'*'):\n",
    "#            images.append(imageio.imread(\"./imgs/\"+file))\n",
    "#    imageio.mimsave('./mvs/'+f+'.gif', images, duration=0.10)\n",
    "#    clip = mp.VideoFileClip(\"./mvs/\"+f+\".gif\")\n",
    "#    clip.write_videofile(\"./mvs/\"+f+\".mp4\")\n",
    "#print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
